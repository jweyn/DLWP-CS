{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DLWP-CS model\n",
    "\n",
    "Now we use the data processed in the previous two notebooks to train a convolutional neural network on weather data mapped to the cubed sphere. We will construct the same convolutional neural network with the cubed sphere as in *Weyn et al. (2020)*, with the exception of having only two variables (Z500 and T2) instead of their four, and without the constant land-sea mask and topography data. This will seem like a fairly involved example but much simpler constructions are also possible using the `DLWPNeuralNet` class instead of the functional Keras API. I also highly recommend having this model train on a GPU with at least 4 GB of video memory.\n",
    "\n",
    "#### Required packages\n",
    "\n",
    "No new packages are needed here beyond the main DLWP-CS requirements in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Let's start with some basic user-selected parameters, beginning with the file paths, which you'll need to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_directory = '/home/wave2/jweyn/Data'\n",
    "predictor_file = os.path.join(root_directory, 'ERA5', 'tutorial_z500_t2m_CS.nc')\n",
    "model_file = os.path.join(root_directory, 'dlwp-cs_tutorial')\n",
    "log_directory = os.path.join(root_directory, 'logs', 'dlwp-cs_tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters:  \n",
    "- `cnn_model_name`: name of the function which constructs the model  \n",
    "- `base_filter_number`: number of convolutional filters in the first convolutional layer  \n",
    "- `min_epochs`: minimum number of training epochs  \n",
    "- `max_epochs`: maximum number of training epochs  \n",
    "- `patience`: if the validation loss does not go down for this number of epochs, end training  \n",
    "- `batch_size`: for mini-batches for SGD training  \n",
    "- `shuffle`: if True, shuffles the training data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_name = 'unet2'\n",
    "base_filter_number = 32\n",
    "min_epochs = 0\n",
    "max_epochs = 20\n",
    "patience = 2\n",
    "batch_size = 32\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable selection. This follows the coordinates in the data file. Make sure to use lists to avoid losing dimensions. Set `add_solar` to True to include insolation variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_selection = {'varlev': ['z/500', 't2m/0']}\n",
    "add_solar = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters govern the time stepping in the model.  \n",
    "- `io_time_steps`: the number of input/output time steps directly ingested/predicted by the model  \n",
    "- `integration_steps`: the number of forward sequence steps on which to minimize the loss function of the model  \n",
    "- `data_interval`: the number of steps in the data file that constitute a \"time step.\" Here we use 2, and the data contains data every 3 hours, so the effective time step is 6 h.\n",
    "- `loss_by_step`: either None (equal weighting) or a list of weighting factors for the loss function at each integration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_time_steps = 2\n",
    "integration_steps = 2\n",
    "data_interval = 2\n",
    "loss_by_step = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the data used for validation and training. Here we use 2013-14 for training and 2015-16 for validation, leaving aside 2017-18 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set = list(pd.date_range('2013-01-01', '2014-12-31 21:00', freq='3H'))\n",
    "validation_set = list(pd.date_range('2015-01-01', '2016-12-31 21:00', freq='3H'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a newer Nvidia GPU (Tesla V100, GeForce RTX series) with tensor cores, you can enable mixed precision for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mp_optimizer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DLWP model\n",
    "\n",
    "Since the data generators depend on the model (granted it's an outdated dependency), we make the model instance first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import DLWPFunctional\n",
    "\n",
    "dlwp = DLWPFunctional(is_convolutional=True, time_dim=io_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and create data generators\n",
    "\n",
    "DLWP-CS includes powerful data generators that produce batches of training data on-the-fly. This enables them to load only the time series into memory instead of repetitive samples of data. On the downside, it makes reading training data from disk virtually impossibly slow. First, load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "data = xr.open_dataset(predictor_file)\n",
    "train_data = data.sel(sample=train_set)\n",
    "validation_data = data.sel(sample=validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training data generator. Here we use the `ArrayDataGenerator` class, which has a nifty pre-processing function to create a single numpy array of data. The `SeriesDataGenerator` is more intuitive and would work equally well. The only reason I don't use the latter is because I thought the overhead when using xarray objects instead of pure numpy might slow things down. It doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model.preprocessing import prepare_data_array\n",
    "from DLWP.model import ArrayDataGenerator\n",
    "\n",
    "print('Loading data to memory...')\n",
    "train_array, input_ind, output_ind, sol = prepare_data_array(train_data, input_sel=io_selection,\n",
    "                                                             output_sel=io_selection, add_insolation=add_solar)\n",
    "generator = ArrayDataGenerator(dlwp, train_array, rank=3, input_slice=input_ind, output_slice=output_ind,\n",
    "                               input_time_steps=io_time_steps, output_time_steps=io_time_steps,\n",
    "                               sequence=integration_steps, interval=data_interval, insolation_array=sol,\n",
    "                               batch_size=batch_size, shuffle=shuffle, channels_last=True,\n",
    "                               drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading validation data to memory...')\n",
    "val_array, input_ind, output_ind, sol = prepare_data_array(validation_data, input_sel=io_selection,\n",
    "                                                           output_sel=io_selection, add_insolation=add_solar)\n",
    "val_generator = ArrayDataGenerator(dlwp, val_array, rank=3, input_slice=input_ind, output_slice=output_ind,\n",
    "                                   input_time_steps=io_time_steps, output_time_steps=io_time_steps,\n",
    "                                   sequence=integration_steps, interval=data_interval, insolation_array=sol,\n",
    "                                   batch_size=batch_size, shuffle=False, channels_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TensorFlow 2.0, using `multiprocessing` in training Keras models has bad behavior (memory leaks). Therefore, for better performance, I recommend using the `tf_data_generator` function to create a `tensorflow.data.Dataset` generator object. It does require knowing the names of the inputs and outputs to use this, however, so we hack the names here. This will become more evident in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import tf_data_generator\n",
    "\n",
    "input_names = ['main_input'] + ['solar_%d' % i for i in range(1, integration_steps)]\n",
    "tf_train_data = tf_data_generator(generator, batch_size=batch_size, input_names=input_names)\n",
    "tf_val_data = tf_data_generator(val_generator, input_names=input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CNN model architecture\n",
    "\n",
    "Now the fun part! Here we create all of the layers that will go into the model. A few notes:  \n",
    "- The generator produces a list of inputs when `integration_steps` is greater than 1:  \n",
    "  - main input, including insolation  \n",
    "  - insolation for step 2  \n",
    "  - insolation for step 3...  \n",
    "- We use our custom layers for padding and convolutions on the cubed sphere  \n",
    "- We can use the Keras 3D layers for operations on the 3D spatial structure of the cubed sphere\n",
    "- There are more layers defined here than actually used in the model architecture. That's ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, UpSampling3D, AveragePooling3D, concatenate, ReLU, Reshape, Concatenate, \\\n",
    "    Permute\n",
    "from DLWP.custom import CubeSpherePadding2D, CubeSphereConv2D\n",
    "\n",
    "# Some shortcut variables. The generator provides the expected shape of the data.\n",
    "cs = generator.convolution_shape\n",
    "cso = generator.output_convolution_shape\n",
    "input_solar = (integration_steps > 1 and add_solar)\n",
    "\n",
    "# Define layers. Must be defined outside of model function so we use the same weights at each integration step.\n",
    "main_input = Input(shape=cs, name='main_input')\n",
    "if input_solar:\n",
    "    solar_inputs = [Input(shape=generator.insolation_shape, name='solar_%d' % d) for d in range(1, integration_steps)]\n",
    "cube_padding_1 = CubeSpherePadding2D(1, data_format='channels_last')\n",
    "pooling_2 = AveragePooling3D((1, 2, 2), data_format='channels_last')\n",
    "up_sampling_2 = UpSampling3D((1, 2, 2), data_format='channels_last')\n",
    "relu = ReLU(negative_slope=0.1, max_value=10.)\n",
    "conv_kwargs = {\n",
    "    'dilation_rate': 1,\n",
    "    'padding': 'valid',\n",
    "    'activation': 'linear',\n",
    "    'data_format': 'channels_last'\n",
    "}\n",
    "skip_connections = 'unet' in cnn_model_name.lower()\n",
    "conv_2d_1 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_1_2 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_1_3 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_2 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_2_2 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_2_3 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_3 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_3_2 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_4 = CubeSphereConv2D(base_filter_number * 4 if skip_connections else base_filter_number * 8, 3, **conv_kwargs)\n",
    "conv_2d_4_2 = CubeSphereConv2D(base_filter_number * 8, 3, **conv_kwargs)\n",
    "conv_2d_5 = CubeSphereConv2D(base_filter_number * 2 if skip_connections else base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_5_2 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_5_3 = CubeSphereConv2D(base_filter_number * 4, 3, **conv_kwargs)\n",
    "conv_2d_6 = CubeSphereConv2D(base_filter_number if skip_connections else base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_6_2 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_6_3 = CubeSphereConv2D(base_filter_number * 2, 3, **conv_kwargs)\n",
    "conv_2d_7 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_7_2 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_7_3 = CubeSphereConv2D(base_filter_number, 3, **conv_kwargs)\n",
    "conv_2d_8 = CubeSphereConv2D(cso[-1], 1, name='output', **conv_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually create the output using the functional API. For each operation in the model, we call the appropriate layer on an input tensor `x`. This function performs the operations inside a U-Net, including the skipped connections with concatenation along the channels dimension. This is the sequence of operations to get input data to a prediction, but it is not the whole model, since that one must predict a sequence of 2 (`integration_steps = 2`). That will be next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet2(x):\n",
    "    x0 = cube_padding_1(x)\n",
    "    x0 = relu(conv_2d_1(x0))\n",
    "    x0 = cube_padding_1(x0)\n",
    "    x0 = relu(conv_2d_1_2(x0))\n",
    "    x1 = pooling_2(x0)\n",
    "    x1 = cube_padding_1(x1)\n",
    "    x1 = relu(conv_2d_2(x1))\n",
    "    x1 = cube_padding_1(x1)\n",
    "    x1 = relu(conv_2d_2_2(x1))\n",
    "    x2 = pooling_2(x1)\n",
    "    x2 = cube_padding_1(x2)\n",
    "    x2 = relu(conv_2d_5_2(x2))\n",
    "    x2 = cube_padding_1(x2)\n",
    "    x2 = relu(conv_2d_5(x2))\n",
    "    x2 = up_sampling_2(x2)\n",
    "    x = concatenate([x2, x1], axis=-1)\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_6_2(x))\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_6(x))\n",
    "    x = up_sampling_2(x)\n",
    "    x = concatenate([x, x0], axis=-1)\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_7(x))\n",
    "    x = cube_padding_1(x)\n",
    "    x = relu(conv_2d_7_2(x))\n",
    "    x = conv_2d_8(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we manipulate the result of the CNN back to inputs to the same CNN, add the new insolation input, and pass it through again. This allows us to minimize the loss function at each step of the sequence. Adding the insolation looks complicated because the array includes a time dimension whereas the data inputs are flattened time/variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape, Concatenate, Permute\n",
    "\n",
    "def complete_model(x_in):\n",
    "    outputs = []\n",
    "    model_function = globals()[cnn_model_name]\n",
    "    is_seq = isinstance(x_in, (list, tuple))\n",
    "    xi = x_in[0] if is_seq else x_in\n",
    "    outputs.append(model_function(xi))\n",
    "    for step in range(1, integration_steps):\n",
    "        xo = outputs[step - 1]\n",
    "        if is_seq and input_solar:\n",
    "            xo = Reshape(cs[:-1] + (io_time_steps, -1))(xo)\n",
    "            xo = Concatenate(axis=-1)([xo, Permute((2, 3, 4, 1, 5))(x_in[step])])\n",
    "            xo = Reshape(cs)(xo)\n",
    "        outputs.append(model_function(xo))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use our known inputs to get the outputs from `complete_model` and construct a Keras Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "if not input_solar:\n",
    "    inputs = main_input\n",
    "else:\n",
    "    inputs = [main_input]\n",
    "    if input_solar:\n",
    "        inputs = inputs + solar_inputs\n",
    "model = Model(inputs=inputs, outputs=complete_model(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function and an optimizer. We use the default mean-squared-error and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "loss_function = 'mse'\n",
    "\n",
    "# Get an optimizer, with mixed precision if requested\n",
    "opt = Adam()\n",
    "if use_mp_optimizer:\n",
    "    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to compile our DLWP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlwp.build_model(model, loss=loss_function, loss_weights=loss_by_step, optimizer=opt, metrics=['mae'])\n",
    "print(dlwp.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the DLWP model\n",
    "\n",
    "Now let's train the model. First, define a few callbacks:\n",
    "- `history`: save the model metrics as it trains  \n",
    "- `early`: a custom callback used to stop training after `patience` epochs, but only after a minimum number of epochs  \n",
    "- `tensorboard`: TensorFlow's complete logging  \n",
    "- `GeneratorEpochEnd`: when using `tf_data_generator`, this is used to shuffle the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import History, TensorBoard\n",
    "from DLWP.custom import EarlyStoppingMin, SaveWeightsOnEpoch, GeneratorEpochEnd\n",
    "\n",
    "history = History()\n",
    "early = EarlyStoppingMin(monitor='val_loss' if validation_data is not None else 'loss', min_delta=0.,\n",
    "                         min_epochs=min_epochs, max_epochs=max_epochs, patience=patience,\n",
    "                         restore_best_weights=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=log_directory, update_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "dlwp.fit_generator(tf_train_data, epochs=max_epochs + 1,\n",
    "                   verbose=1, validation_data=tf_val_data,\n",
    "                   callbacks=[history, early, save, GeneratorEpochEnd(generator)])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model. We use the DLWP utility function, which saves the DLWP instance as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.util import save_model\n",
    "\n",
    "save_model(dlwp, model_file, history=history)\n",
    "print('Wrote model %s' % model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print some loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTrain time -- %s seconds --\" % (end_time - start_time))\n",
    "\n",
    "score = dlwp.evaluate(*val_generator.generate([]), verbose=0)\n",
    "print('Validation loss:', score[0])\n",
    "print('Other scores:', score[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! You have successfully trained a pure machine-learning global weather prediction model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
